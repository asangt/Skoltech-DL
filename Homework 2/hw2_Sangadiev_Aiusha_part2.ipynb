{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "gFe273vKFgF-"
   },
   "source": [
    "# Homework 2, *part 2*\n",
    "### (60 points total)\n",
    "\n",
    "In this part, you will build a convolutional neural network (CNN) to solve (yet another) image classification problem: the Tiny ImageNet dataset (200 classes, 100K training images, 10K validation images). Try to achieve as high accuracy as possible.\n",
    "\n",
    "**Unlike part 1**, you are now free to use the full power of PyTorch and its subpackages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKjuxXAgFgGA"
   },
   "source": [
    "## Deliverables\n",
    "\n",
    "* This file.\n",
    "* A \"checkpoint file\" `\"checkpoint.pth\"` that contains your CNN's weights (you get them from `model.state_dict()`). Obtain it with `torch.save(..., \"checkpoint.pth\")`. When grading, we will load it to evaluate your accuracy.\n",
    "\n",
    "**Should you decide to put your `\"checkpoint.pth\"` on Google Drive, update (edit) the following cell with the link to it:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O4kOWlcXFgGD"
   },
   "source": [
    "### [Dear TAs, I've put my \"checkpoint.pth\" on Google Drive, download it here](https://drive.google.com/open?id=1unnXVbB-vK6-tW87NyYOdwJo39p_rLR6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-S-WyYK8FgGF"
   },
   "source": [
    "## Grading\n",
    "\n",
    "* 9 points for reproducible training code and a filled report below.\n",
    "* 11 points for building a network that gets above 25% accuracy.\n",
    "* 4 points for using an **interactive** (please don't reinvent the wheel with `plt.plot`) tool for viewing progress, for example Tensorboard ([with this library](https://github.com/lanpa/tensorboardX) and [an extra hack for Colab](https://stackoverflow.com/a/57791702)). In this notebook, insert screenshots of accuracy and loss plots (training and validation) over iterations/epochs/time.\n",
    "* 6 points for beating each of these accuracy milestones on the private **test** set:\n",
    "  * 30%\n",
    "  * 34%\n",
    "  * 38%\n",
    "  * 42%\n",
    "  * 46%\n",
    "  * 50%\n",
    "  \n",
    "*Private test set* means that you won't be able to evaluate your model on it. Rather, after you submit code and checkpoint, we will load your model and evaluate it on that test set ourselves, reporting your accuracy in a comment to the grade.\n",
    "\n",
    "Note that there is an important formatting requirement, see below near \"`DO_TRAIN = True`\".\n",
    "\n",
    "## Restrictions\n",
    "\n",
    "* No pretrained networks.\n",
    "* Don't enlarge images (e.g. don't resize them to $224 \\times 224$ or $256 \\times 256$).\n",
    "\n",
    "## Tips\n",
    "\n",
    "* **One change at a time**: never test several new things at once (unless you are super confident). Train a model, introduce one change, train again.\n",
    "* Google a lot: try to reinvent as few wheels as possible (unlike in part 1 of this assignment).\n",
    "* Use GPU.\n",
    "* Use regularization: L2, batch normalization, dropout, data augmentation...\n",
    "* Pay much attention to accuracy and loss graphs (e.g. in Tensorboard). Track failures early, stop bad experiments early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "m2qxvLGdFgGH",
    "outputId": "a33d4017-35fe-4821-9554-6eb0561e0d71"
   },
   "outputs": [],
   "source": [
    "# Detect if we are in Google Colaboratory\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "from pathlib import Path\n",
    "# Determine the locations of auxiliary libraries and datasets.\n",
    "# `AUX_DATA_ROOT` is where 'notmnist.py', 'animation.py' and 'tiny-imagenet-2020.zip' are.\n",
    "if IN_COLAB:\n",
    "    google.colab.drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Change this if you created the shortcut in a different location\n",
    "    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/Deep Learning 2020 -- Home Assignment 2\")\n",
    "    \n",
    "    assert AUX_DATA_ROOT.is_dir(), \"Have you forgot to 'Add a shortcut to Drive'?\"\n",
    "else:\n",
    "    AUX_DATA_ROOT = Path(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_6veP_5FgGS"
   },
   "source": [
    "The below cell puts training and validation images in `./tiny-imagenet-200/train` and `./tiny-imagenet-200/val`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njk50aDoFgGT"
   },
   "outputs": [],
   "source": [
    "# Extract the dataset into the current directory\n",
    "if not Path(\"tiny-imagenet-200/train/class_000/00000.jpg\").is_file():\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(AUX_DATA_ROOT / 'tiny-imagenet-2020.zip', 'r') as archive:\n",
    "        archive.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1mtHCQcFgGb"
   },
   "source": [
    "**You are required** to format your notebook cells so that `Run All` on a fresh notebook:\n",
    "* trains your model from scratch, if `DO_TRAIN is True`;\n",
    "* loads your trained model from `\"./checkpoint.pth\"`, then **computes** and prints its validation accuracy, if `DO_TRAIN is False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QvH36LxFgGc"
   },
   "outputs": [],
   "source": [
    "DO_TRAIN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rgg4D0zSFgGi"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B-oK-doYeE69"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torch_data\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sNzHKE_Ad2HP",
    "outputId": "c4d3cf04-35f8-42fb-a850-b70fdf0136d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: GeForce RTX 2070 (UUID: GPU-69831a40-2410-16e9-5f2b-087be136b9f4)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "id": "eH9bgsJ2Ulhu",
    "outputId": "2bcd95d3-b0d2-4cfa-bfa9-086efa4910c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# WandB – Install the W&B library\n",
    "%pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sL9VylXBWZsP"
   },
   "outputs": [],
   "source": [
    "# Ignore excessive warnings\n",
    "import logging\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# WandB – Import the wandb library\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YazcoH7Vu_xx"
   },
   "outputs": [],
   "source": [
    "class Conv2dBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Full pre-activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size  = kernel_size\n",
    "        self.padding      = (self.kernel_size // 2, self.kernel_size // 2)\n",
    "        self.stride       = stride\n",
    "\n",
    "        self.layer_block  = nn.Sequential( \n",
    "                                          nn.BatchNorm2d(self.in_channels),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Conv2d(self.in_channels, self.out_channels,\\\n",
    "                                                    kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, bias=False)\n",
    "                                         )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipG0KOLa8A8F"
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, block_size, downsampling=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.block_size   = block_size\n",
    "\n",
    "        self.downsampling = downsampling\n",
    "        if self.downsampling:\n",
    "            self.stride = 2\n",
    "        else:\n",
    "            self.stride = 1\n",
    "\n",
    "        layers = [Conv2dBlock(self.out_channels, self.out_channels, 3) for i in range(1, self.block_size)]\n",
    "\n",
    "        self.block = nn.Sequential( \n",
    "                                   Conv2dBlock(self.in_channels, self.out_channels, 3, stride=self.stride),\n",
    "                                   *layers\n",
    "                                  )\n",
    "        \n",
    "        self.shortcut = nn.Sequential( \n",
    "                                      nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=self.stride),\n",
    "                                      nn.BatchNorm2d(self.out_channels)\n",
    "                                     )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.in_channels != self.out_channels:\n",
    "            identity = self.shortcut(x)\n",
    "        else:\n",
    "            identity = x\n",
    "\n",
    "        x  = self.block(x)\n",
    "        x += identity\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFqnb1-EFgGj"
   },
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(ResNet34, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = Conv2dBlock(self.in_channels, 64, 3, stride=1)\n",
    "        self.conv2 = nn.Sequential(\n",
    "                                    #nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                                    ResBlock(64, 64, 6)\n",
    "                                  )\n",
    "        self.conv3 = ResBlock(64, 128, 8, downsampling=True)\n",
    "        self.conv4 = ResBlock(128, 256, 12, downsampling=True)\n",
    "        self.conv5 = ResBlock(256, 512, 6, downsampling=True)\n",
    "\n",
    "        self.fc_out = nn.Sequential( \n",
    "                                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "                                    nn.Flatten(),\n",
    "                                    nn.Linear(512, self.num_classes),\n",
    "                                    nn.LogSoftmax(dim=1)\n",
    "                                   )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MyDIt1Rc4zIr"
   },
   "outputs": [],
   "source": [
    "def train(net, n_epochs, optimizer, criterion, train_loader, val_loader, device,\\\n",
    "          scheduler=None, early_stop=None, save_dir=None):\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_loss, val_loss = 0.0, 0.0\n",
    "        train_correct, val_correct = 0, 0\n",
    "\n",
    "        # training\n",
    "        net.train()\n",
    "        for X, y in train_loader:\n",
    "            # send data to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # make prediction and calculate loss\n",
    "            y_prob = net(X)\n",
    "            loss   = criterion(y_prob, y)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # evaluate accuracy\n",
    "            y_pred = y_prob.argmax(dim=1)\n",
    "            train_correct += (y_pred == y).sum().item()\n",
    "\n",
    "            # update model weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            del loss\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # validation\n",
    "        net.eval()\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_prob = net(X)\n",
    "            loss   = criterion(y_prob, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # evaluate accuracy\n",
    "            y_pred = y_prob.argmax(dim=1)\n",
    "            val_correct += (y_pred == y).sum().item()\n",
    "\n",
    "            del loss\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        if early_stop is not None:\n",
    "            early_stop.step()\n",
    "\n",
    "        # log results\n",
    "        wandb.log({\n",
    "                    \"Train loss\" : train_loss,\n",
    "                    \"Validation loss\" : val_loss,\n",
    "                    \"Train accuracy\" : 100 * train_correct / len(train_loader.dataset),\n",
    "                    \"Validation accuracy\" : 100 * val_correct / len(val_loader.dataset)\n",
    "                 })\n",
    "    \n",
    "    if save_dir is not None:\n",
    "        torch.save(net, save_dir)\n",
    "        wandb.save(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQPW4Ez0W8UQ"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "                                      transforms.RandomCrop(56),\n",
    "                                      transforms.ColorJitter(brightness=(0.25, 1.5), saturation=(0.25, 1.5)),\n",
    "                                      transforms.RandomChoice([transforms.RandomHorizontalFlip(),\n",
    "                                                               transforms.RandomVerticalFlip()]),\n",
    "                                      transforms.RandomAffine(degrees=20, scale=(0.8, 1.1), shear=10),\n",
    "                                      transforms.ToTensor()\n",
    "                                    ])\n",
    "val_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dset = torchvision.datasets.ImageFolder(root='./tiny-imagenet-200/train', transform=train_transform)\n",
    "val_dset   = torchvision.datasets.ImageFolder(root='./tiny-imagenet-200/val', transform=val_transform)\n",
    "im_channels, num_classes = 3, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWqSkvePYByQ"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Qh3LoyFVqDCF",
    "outputId": "acd6b9d6-4011-4f07-c069-0942c632ce0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Weights & Biases!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\asang/.netrc\n"
     ]
    }
   ],
   "source": [
    "# WandB – Login to your wandb account so you can log all your metrics\n",
    "!wandb login 031af70dba88e746696d15cc5bdddf1dc268ab62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWmQ2rmbEUoj"
   },
   "outputs": [],
   "source": [
    "def fix_seed(seed=0, device=device):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device.type == 'cuda':\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "colab_type": "code",
    "id": "WWcZ2RXNOWbv",
    "outputId": "4d0849e4-cece-44d4-b906-cc5e4d202765"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 22 20:56:15 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 441.12       Driver Version: 441.12       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 2070   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   39C    P8     7W /  N/A |   3209MiB /  8192MiB |     14%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1204    C+G   Insufficient Permissions                   N/A      |\n",
      "|    0      2304    C+G   ...ogram Files\\Mozilla Firefox\\firefox.exe N/A      |\n",
      "|    0      3744      C   C:\\Users\\asang\\anaconda3\\python.exe        N/A      |\n",
      "|    0      4684    C+G   ...ogram Files\\Mozilla Firefox\\firefox.exe N/A      |\n",
      "|    0      5364    C+G   ...t_cw5n1h2txyewy\\ShellExperienceHost.exe N/A      |\n",
      "|    0      5660    C+G   ...ogram Files\\Mozilla Firefox\\firefox.exe N/A      |\n",
      "|    0      7476    C+G   ...dows.Cortana_cw5n1h2txyewy\\SearchUI.exe N/A      |\n",
      "|    0      7660    C+G   ...ppData\\Local\\TIDAL\\app-2.17.0\\TIDAL.exe N/A      |\n",
      "|    0      7692    C+G   ...common\\wallpaper_engine\\wallpaper64.exe N/A      |\n",
      "|    0      7856    C+G   C:\\Windows\\explorer.exe                    N/A      |\n",
      "|    0      8968    C+G   ...5n1h2txyewy\\StartMenuExperienceHost.exe N/A      |\n",
      "|    0      9800    C+G   ...mmersiveControlPanel\\SystemSettings.exe N/A      |\n",
      "|    0      9972    C+G   ...hell.Experiences.TextInput.InputApp.exe N/A      |\n",
      "|    0     10144    C+G   ....0_x64__v10z8vjag6ke6\\HP.JumpStarts.exe N/A      |\n",
      "|    0     10216    C+G   ...1.91.0_x64__8wekyb3d8bbwe\\YourPhone.exe N/A      |\n",
      "|    0     10508    C+G   ...osoft.LockApp_cw5n1h2txyewy\\LockApp.exe N/A      |\n",
      "|    0     11432    C+G   ...6\\win32\\OmenCommandCenterBackground.exe N/A      |\n",
      "|    0     11616    C+G   ...entUtility\\HPSystemEventUtilityHost.exe N/A      |\n",
      "|    0     11836    C+G   ...ogram Files\\Mozilla Firefox\\firefox.exe N/A      |\n",
      "|    0     12232    C+G   ...e6\\win32\\CefSharp.BrowserSubprocess.exe N/A      |\n",
      "|    0     13388    C+G   ...1.0_x64__8wekyb3d8bbwe\\WinStore.App.exe N/A      |\n",
      "|    0     13660    C+G   ... Files (x86)\\Dropbox\\Client\\Dropbox.exe N/A      |\n",
      "|    0     14280    C+G   ...oftEdge_8wekyb3d8bbwe\\MicrosoftEdge.exe N/A      |\n",
      "|    0     14772    C+G   ...ogram Files\\Mozilla Firefox\\firefox.exe N/A      |\n",
      "|    0     15244    C+G   ...ke6\\win32\\HP.Omen.OmenCommandCenter.exe N/A      |\n",
      "|    0     15880    C+G   ...ogram Files\\Mozilla Firefox\\firefox.exe N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9n7DyGcFgGq"
   },
   "outputs": [],
   "source": [
    "if DO_TRAIN:\n",
    "    wandb.init(project=\"skoltech_dl_hw2\")\n",
    "\n",
    "    # dataloaders\n",
    "    batch_size = 128\n",
    "    train_loader = torch_data.DataLoader(train_dset, batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader   = torch_data.DataLoader(val_dset, batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    # fix seeds for reproducibility\n",
    "    fix_seed(666, device)\n",
    "\n",
    "    # test loader\n",
    "    for X, y in train_loader:\n",
    "        print(X[0].size(), y[0])\n",
    "        plt.imshow(X[0,0,:,:].numpy())\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "    # model\n",
    "    net = ResNet34(im_channels, num_classes)\n",
    "\n",
    "    # training parameters\n",
    "    n_epochs = 30\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True, threshold=0.01)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    print(\"Begin training...\")\n",
    "    net.to(device)\n",
    "    train(net, n_epochs, optimizer, criterion, train_loader, val_loader, device, scheduler, save_dir='checkpoint.pth')\n",
    "    print(\"Training successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHFdDYv8FgGx"
   },
   "source": [
    "## Load and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gh0c-YTRFEWm"
   },
   "outputs": [],
   "source": [
    "def validate(net, criterion, val_loader, device):\n",
    "    val_loss, val_acc = 0.0, 0\n",
    "    for X, y in val_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        y_prob    = net(X)\n",
    "        val_loss += criterion(y_prob, y).item()\n",
    "\n",
    "        # accuracy\n",
    "        y_pred = y_prob.argmax(dim=1)\n",
    "        val_acc += (y_pred == y).sum().item()\n",
    "    return val_loss / len(val_loader), 100 * val_acc / len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-YNjqEMFgGy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here (load the model from \"./checkpoint.pth\")\n",
    "# Please use `torch.load(\"checkpoint.pth\", map_location='cpu')`\n",
    "\n",
    "model = ResNet34(im_channels, num_classes)\n",
    "model.load_state_dict(torch.load(\"checkpoint.pth\", map_location='cpu'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VYKCk2rnFgG2",
    "outputId": "3a3b698c-d0ee-461d-f18c-8262ed69a963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 57.64%\n"
     ]
    }
   ],
   "source": [
    "# In case GPU has smaller memory, use smaller batch size\n",
    "batch_size   = 32\n",
    "val_loader   = torch_data.DataLoader(val_dset, batch_size, shuffle=False)\n",
    "\n",
    "val_accuracy = validate(model, criterion, val_loader, device)[1]\n",
    "assert 0 <= val_accuracy <= 100\n",
    "print(\"Validation accuracy: %.2f%%\" % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crMQ_SvcFgG7"
   },
   "source": [
    "# Report\n",
    "\n",
    "Below, please mention:\n",
    "\n",
    "* A brief history of tweaks and improvements.\n",
    "* Which network architectures have you tried? What is the final one and why?\n",
    "* What is the training method (batch size, optimization algorithm, number of iterations, ...) and why?\n",
    "* Which techniques have you tried to prevent overfitting? What were their effects? Which of them worked well?\n",
    "* Any other insights you learned.\n",
    "\n",
    "For example, start with:\n",
    "\n",
    "\"I have analyzed these and those conference papers/sources/blog posts. \\\n",
    "I tried this and that to adapt them to my problem. \\\n",
    "The conclusions this task taught me are ...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBcPj7EyFgG8"
   },
   "source": [
    "### Initial network architecture\n",
    "First of all, I started with analyzing this paper by A. Canziani et al. (https://arxiv.org/abs/1605.07678), where the authors compare different CNN architectures in terms of computing time and performance on the ImageNet dataset (top-1 validation accuracy). Although, in this homework we used a different version of it, I thought that this paper might give a general idea about the networks. As a result, I've chosen a **ResNet34** architecture, which offers both a reasonable computational cost and good accuracy.<br>\n",
    "Secondly, I've read the original paper on ResNets by K. He et al. (https://arxiv.org/abs/1512.03385), where they presented their idea on residual learning and different ResNet architectures, in order to understand how to implement the chosen ResNet34 network.<br>\n",
    "While implementing of the network, I used `torchvision` ResNet implementation as a reference point when some things were unclear.\n",
    "\n",
    "### Training method and parameters\n",
    "Final training parameters (for the latest version of the network) are as follows:<br>\n",
    "1) `batch_size = 128` - mainly because the larger batch size which I used in the beginning (`batch_size = 256`) was giving \"out of memory\" error and also because this number was frequently used in the papers \\ reports regarding the performnace on Tiny ImageNet dataset, and is reasonable from the computational time point;<br>\n",
    "2) initially I trained the networks using ADAM optimizer, but it was only working well with `learning_rate = 1e-3` and its convergence was actually worse than the simple SGD with Nesterov momentum. Hence, I've chosen to use SGD with Momentum with `learning_rate = 1e-2`, `momentum = 0.9`, and `weight_decay = 1e-4` (for $L_2$ regularization);<br>\n",
    "3) I also used a learning rate scheduler - if the loss on validation set did not improve (compared to the best achieved validation loss) for 3 epochs in a row, then learning rate was decayed through multiplication by $ \\gamma = 0.1 $;<br>\n",
    "4) number of epochs `n_epochs = 30` was chosen empirically, by observing that usually during training, the validation loss stops improving significantly after the first learning rate decay which takes places at around 10-20 epochs into the training. This can also be seen on the following plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0ksw-mOvqWJ"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1m9c4E9pl6dJRHp0OPoTv9tDuI9z8Mdf1\" width=800>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SmPgI6RQUjY"
   },
   "source": [
    "As can be seen from the plot, the validation loss almost stops improving after around 22 epochs into the training. The rapid drop after the 20th epoch indicates the first learning rate decay;<br>\n",
    "5) negative log likelihood (NLL) loss function was used for both training and validation, as it is one of the most widely used loss functions for multiclass classification.<br>\n",
    "\n",
    "### Preventing overfitting\n",
    "\n",
    "1) to begin with, larger models with many trainable parameters are usually more prone to overfitting and \"remembering\" the input data, so one of the first decisions was to choose models with reasonable number of parameters. While all popular networks (AlexNet, VGG, GoogleNet, Inception, ResNet etc.) have a reasonable performance on the full dataset, in the case of this homework, we are dealing with a much smaller dataset of 100'000 images compared to over 14 million (as per wiki - https://en.wikipedia.org/wiki/ImageNet) images in the original dataset. Hence, it was one more reason to choose ResNet34 with around 21.4 million parameters compared to e.g. VGG-16 with 138 (!) million;<br>\n",
    "2) continuing the topic of a smaller dataset, it should be noted that we have only 500 training images per 200 classes. Even smaller networks can start overfitting heavily after a few training epochs (as evidenced in this report - http://cs231n.stanford.edu/reports/2017/pdfs/12.pdf), hence we need to find a way to \"enlarge\" our dataset and one of the main tricks there is data augmentation. Basing on these sources - the aforementioned report, blogpost (https://learningai.io/projects/2017/06/29/tiny-imagenet.html), one more Stanford report (http://cs231n.stanford.edu/reports/2017/pdfs/931.pdf) - I've chosen to implement the following random changes to the images: random crop to 56x56 pixels (from 64x64), brightness and saturation jitters, random horizontal \\ vertical flips, random affine transformations (rotation, shear, and scaling). Also, when converting to tensors, Pytorch automatically normalizes pixel intensities to be in the range of [0, 1].Data augmentation helped to reduce overfitting and increased the validation accuracy from ~35% to 38% with the original ResNet34 network;<br>\n",
    "3) one more technique to combat overfitting was usage of $L_2$ regularization (implemented through weight decay in the optimizer in PyTorch), which is one of the most widespread ways on reducing the overfitting problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ABnifoMGQKjF"
   },
   "source": [
    "### Corrections to the original network\n",
    "1) **Original ResNet34** - I was able to achieve about ~38% validation accuracy after 30 epochs with the original ResNet34 structure.<br>\n",
    "2) **Full pre-activation ResNet34** - in order to increase the accuracy, I analyzed the following work by the authors of the original paper on ResNets - https://arxiv.org/pdf/1603.05027.pdf. There they propose a different structure of a ResNet block - in the original paper the structure is as follows: CONV -> BatchNorm -> ReLU -> CONV -> BatchNorm -> Add -> ReLU, in the aforementioned paper, they instead propose several different schemes and conclude that full pre-activation scheme: BatchNorm -> ReLU -> CONV -> BatchNorm -> ReLU -> CONV provides a better accuracy, although this change is significant mostly on the larger networks. I've implemented the full pre-activation scheme and saw a 1% increase in the validation accuracy after 30 epochs, leading to ~39% accuracy.<br>\n",
    "3) **Tuned ResNet34** - inspired by the analysis of the dataset in the following report (http://cs231n.stanford.edu/reports/2016/pdfs/411_Report.pdf), which states that one of the main problems with the Tiny ImageNet dataset is that original \"big\" networks are designed to tackle much larger images from the original ImageNet and they rapidly decrease the amount of information in the layers (https://github.com/tjmoon0104/Tiny-ImageNet-Classifier) through usage of big kernels and active downsampling through strides. I've made slight changes to the network architecture - kernel size of the input convolutional layer was change to 3x3 from 7x7, stride was decreased from 2 to 1 (meaning that there is no downsampling). This change made a huge impact as I was able to achieve ~48% validation accuracy, which is 9% higher than the previous result. Thus, I've decided to dig a little bit more in that direction.<br>\n",
    "4) **Final model** - next move was to remove the input max pooling layer, as a result input size of an image to the first ResNet block changed from `16x16x64` in the original ResNet34 (conv1 layer) to `64x64x64`. This increased the memory usage in the GPU, hence I was forced to decrease the batch size from `batch_size = 256` for the 3 networks above to `batch_size = 128`. However, removing the max pooling layer resulted in even better results at 57.64% validation accuracy. The effect of changing batch size should be minimal here, because I've tested smaller batch sizes with the pre-activation ResNet34 and the effect was not significant (accuracy even decreased a bit). Hence, removal of max pooling gives the main accuracy bump here.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m9HTkACsmytq"
   },
   "source": [
    "### Final model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hRUTsunkm1sE",
    "outputId": "32c3a093-8a7e-46c4-ccab-339d8df38694"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0fddd433a43b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet34\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "net = ResNet34(3, 200)\n",
    "summary(net, input_size=(3,64,64), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hjmm9le6mwsM"
   },
   "source": [
    "### Plots for the models 2-4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "usY-bS_ByTyn"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1qPdko_rc6zctJUM_xpd-oWNG0kSPllOe\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLdp1llVyp3s"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=17UUzBHX0XyyKv3bSgfcFcrCzER6UNNJv\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NO0OrK_Oy0LT"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1R5gzuFCUvrpPwIkhNpzidSdz0SgI5fyp\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F3GyZYVcy5fa"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1f8-vdD19kYx49hXJlzBk3PF2kh16yDcS\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x0N8tYDFicoM"
   },
   "source": [
    "The singular sharp drops \\ rises after 20+ epochs are due to the learning rate decay from 0.01 to 0.001. As we can see, final model outperforms previous ones and there is no serious overfitting, as final train accuracy at 30th epoch was ~62.8%, which is comparable to the validation accuracy at 57.64%. Same for the losses - 1.431 for training, 1.733 for validation after 30 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQKkHNTslKpT"
   },
   "source": [
    "### Concluding remarks\n",
    "\n",
    "This was a very interesting, albeit a challenging homework. In the process of doing it, I was able to obtain knowledge in many directions - data augmentations (+ application on practice and their big effect on the training process), different CNN architectures (as a result of doing a little research while choosing a network for this task). Most importantly, I've learned the effect of tuning the network architectures for your own task, which is really huge - making a set of small adaptations to the original ResNet34 architecture increased the validation accuracy by almost 20%!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Part 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
